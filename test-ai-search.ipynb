{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to try AI Search in Azure.\n",
    "\n",
    "See: https://python.langchain.com/v0.2/docs/integrations/vectorstores/azuresearch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv(filename='.env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_api_version = \"2023-05-15\"\n",
    "model = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_address = \"https://ai-search-1-sn.search.windows.net\"\n",
    "vector_store_password = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=openai_api_key, \n",
    "    openai_api_version=openai_api_version, \n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"vector-1723032578840\"\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=index_name,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://api.python.langchain.com/en/latest/_modules/langchain_community/vectorstores/azuresearch.html#AzureSearch.similarity_search\n",
    "# we need to parametrize the fields\n",
    "print(os.environ.get(\"AZURESEARCH_FIELDS_CONTENT_VECTOR\"))\n",
    "print(os.environ.get(\"AZURESEARCH_FIELDS_ID\"))\n",
    "print(os.environ.get(\"AZURESEARCH_FIELDS_CONTENT\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source code for langchain_community.vectorstores.azuresearch\n",
    "https://api.python.langchain.com/en/latest/_modules/langchain_community/vectorstores/azuresearch.html#AzureSearch.similarity_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"What are the fully local agents?\"\n",
    "# query = \"What is Nvidia NIM API?\"\n",
    "query = \"What is a generator function?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity search with relevance scores\n",
    "similarity_docs = vector_store.similarity_search(\n",
    "    query=query,\n",
    "    k=3,\n",
    "    search_type=\"similarity\",\n",
    "    score_threshold=0.79,\n",
    ")\n",
    "print(similarity_docs[0])\n",
    "similarity_doc_string = ' '.join(doc.page_content for doc in similarity_docs[:3])\n",
    "print(len(similarity_doc_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a hybrid search using the hybrid_search method\n",
    "hybrid_docs = vector_store.hybrid_search(\n",
    "    query=query, \n",
    "    k=3\n",
    ")\n",
    "\n",
    "print(hybrid_docs[0])\n",
    "hybrid_doc_string = ' '.join(doc.page_content for doc in hybrid_docs[:3])\n",
    "print(len(hybrid_doc_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Answer this single question {query} basing only on the information below between four hashes. If the information contains other questions ignore them. If possible, give an example related to the answer: #### {info} ####\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell requires local Ollama model\n",
    "llm = ChatOllama(model=\"llama3.1\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Similarity Search\")\n",
    "print(chain.invoke({\"query\": query, \"info\": similarity_doc_string}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hybrid Search\")\n",
    "print(chain.invoke({\"query\": query, \"info\": hybrid_doc_string}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aisearch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
