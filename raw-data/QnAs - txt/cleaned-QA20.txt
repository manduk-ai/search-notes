What is an n-gram?
an n-gram is a contiguous sequence of n items from a given sample of text or speech
####

What is the difference between torch.nn and torch.nn.functional?
torch.nn defines nn.Module classes, the latter uses a functional (stateless) approach.
nn.Modules are defined as Python classes and have attributes, e.g. a nn.Conv2d module 
will have some internal attributes like self.weight. F.conv2d however just defines the operation 
and needs all arguments to be passed (including the weights and bias). 
Internally the modules will usually call their functional counterpart in the forward method somewhere.
####

How to import nn and functional
import torch.nn as nn
import torch.functional as F
####

How to maintain model
There are 2 strategies. 
1. We monitor model to detect drift, we retrain
2. More proactive model - we retrain:
a. Based on time (retrain daily or weekly)
b. Based on data (whenever you collect more data)
c. Based on events (whenever your world changes)
####

What is the dot product?
Algebraic operation that takes two equal-length sequences of numbers 
and returns a single number by multiplying corresponding entries 
and summing them
v5 = torch.tensor([1,3,-5])
v6 = torch.tensor([4,-2,-1])
torch.dot(v5, v6)
>>>	3
in case of 2D matrices we use torch.mm
#columns must be equal to # of rows
A(m x n) * B(n x p)
and then we calculate dot product between rows and columns
####

Difference between PCA and Undercompletr Autoencoder
PCA can only build linear relationships. 
As a result, it is put at a disadvantage compared with 
methods like undercomplete autoencoders 
that can learn non-linear relationships and, therefore, 
perform better in dimensionality reduction.
if we remove all non-linear activations from an undercomplete autoencoder 
and use only linear layers, we reduce the undercomplete autoencoder 
into something that works at an equal footing with PCA.
####

Difference between nn.NNNLoss and CrossEntropyLoss in PyTorch (also binaries)
binary_cross_entropy takes logistic sigmoid values as inputs
binary_cross_entropy_with_logits takes logits as inputs
cross_entropy takes logits as inputs (performs log_softmax internally)
nll_loss is like cross_entropy but takes log-probabilities (log-softmax) values as inputs
####

Autoencoders Usage
Dimensionality Reduction. The Encoder encodes the input into the hidden layer 
to reduce the dimensionality of linear and nonlinear data; hence it is more powerful than PCA.
Recommendation Engines
Anomaly Detection: Autoencoders tries to minimize the reconstruction error 
as part of its training. Anomalies are detected by checking the magnitude of the reconstruction loss.
Denoising Images: An image that is corrupted can be restored to its original version.
Image recognition: Stacked autoencoder are used for image recognition by learning the different features of an image.
Image generation: Variational Autoencoder(VAE), a type of autoencoders, is used to generate images.
