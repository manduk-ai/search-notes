Implement code for sigmoid and its derivative
import numpy as np
def sigmoid(x):
  return 1 / (1 + np.exp(-x))
def deriv_sigmoid(x):
  return sigmoid(x) * (1 - sigmoid(x))
####

Implement code for hiperbolic tangent and its derivative
import numpy as np
def tanh(x):
  return 2 / (1 + np.exp(-2 * x)) - 1
def deriv_tanh(x):
  return 1 - tanh(x) ** 2
####

Implement code for relu and its derivative
import numpy as np
def relu(x):
  return np.max(0,x)
def deriv_relu(x):
  return (x > 0) * 1
####

What's the purpose of training, validation/dev and test datasets?
Training dataset - for training parameters of your model
Validation/dev dataset - for training hyper-parameters of your model
Test dataset - for evaluating performance of the model
####

What is latent space?
In machine learning, we use "latent space" to refer to a multi-dimensional space 
containing a meaningful internal representation of objects and where similar points 
appear closer together.
Latent spaces usually have a lower dimensionality than the feature space used 
to draw specific data points. Because of this, projecting an object in a latent space 
is typically a dimensionality reduction exercise.
The more dimensions in the latent space, the more sensitive it is to specific features 
from the input objects. In other words, small changes in the input data could cause 
significant variations in their representation in latent space. 
This makes high-dimensional spaces more likely to overfit than low-dimensional spaces.
On the other hand, the lower the latent space's dimensionality, the less sensitive 
to small changes in the input data. The more the encoder compresses the data, 
the fewer details will make it into the latent space. Low-dimensional spaces capture 
the essential features of the input data and are more robust to overfitting.
####

Difference betweem Softmax and Log-Softmax
Advantages of using Log-Softmax over Softmax
a) improved numerical performance - no risk of NaN
b) gradient optimization - Gradient methods generally work better optimizing logp(x) than p(x) 
because the gradient of logp(x) is generally more well-scaled
It's worth noting that
Log-Softmax(tensor) ~ Sofmax(tensor - max(tensor))
####

code for counting elements in a list or a list of lists
sum(el.nelement() for el in some_list)
####

What is ROC curve
Receiver Operator Characteristic is a graphical plot used to show 
the diagnostic ability of binary classifiers.
Constructed by plotting the true positive rate (TPR) against the false positive rate (FPR)
TPR is the proportion of observations that were correctly predicted to be positive 
out of all positive observations (TP/(TP + FN))
FPR proportion of observations that are incorrectly predicted to be positive 
out of all negative observations (FP/(TN + FP))
Note that the ROC does not depend on the class distribution. This makes it useful for evaluating 
classifiers predicting rare events such as diseases or disasters. 
In contrast, evaluating performance using accuracy (TP + TN)/(TP + TN + FN + FP) would favor 
classifiers that always predict a negative outcome for rare events.
####

What is RNN
Recurrent neural networks (RNN) are a type of artificial neural network that can process 
sequential or time series data. Their main difference from traditional networks is their ability 
to take information from prior inputs to influence the current input and output. 
This ability allows them to capture any sequential information present in the data. 
For example, an RNN is ideal for capturing the dependency between words of a sentence.
RNN processes the data sequentially so a model can process sequences of varying sizes. 
For example, an RNN can process a 5-word and 10-word sentence using the same input structure, 
unlike a traditional neural network that will need a different input size for each case.
----
RNNs share the same weight parameters for every input sample, unlike traditional networks 
with different weights across each input node. Sharing parameters helps an RNN generalize 
to sequences of varying lengths and operate similarly on sequences with the same meaning 
but organized differently.
####

What is Precision, Accuracy, Recall (sensitivity), Specificity
These are the main metrics used to assess performance of classification models.
-----
Accuracy: how many times the ML model was correct overall. 
Accuracy = (# of correct predictions) / (# of all predictons)
-----
Precision: how good the model is at predicting a specific category. 
Precision =  (true positives) / (all positive predictions for this category = TP + FP)
-----
Recall or Sensitivity: how many times the model was able to detect a specific category.
In other words - True Positive Rate. The capacity of a model to identify positive samples.
Recall or Sensitivity = (TP / (TP + FN)
Higher Recall / sensitivity: Model is good at detecting positive samples.
-----
Specificity: True Negative Rate of the model.
Higher specificity: Model is good at detecting negative samples.
Specificity = (TN) / TN + FP
