Q: Huber Loss 
A: Less sensitive to outliers than squared error loss (SEL)
A: Quadratic for small values (like SEL) and linear for larger
A: It look like failed parabola
A: 1/2*a^2 		for |a| < P
A: P*(|a| - 1/2*P)) 	otherwise

Q: Mean Squared Error in pytorch
A: torch.nn.MSELoss
A: aka L2 Loss
A: measures squarred error that can be reduced using reduction parameter (sum or mean)
A: if reduction is 'none' we need to take care of reduction by ourselves e.g. by summing

Q: Mean Absolute Error in pytorch
A: aka L1 Loss
A: measures absoulte error that can be reduced using reduction parameter (sum or mean)
A: if reduction is 'none' we need to take care of reduction by ourselves e.g. by summing

Q: Root Mean Squarred Error in pytorch
A: Use MSELoss and square it
A: criterion = nn.MSELoss()
A: loss = torch.sqrt(criterion(x, y))

Q: How to chech if GPU in available?
A: device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
A: print(device)

Q: What is nn.functional module and how to import it?
A: The main difference between the nn.functional.xxx and the nn.xxx is 
A: that nn.xxx has a state and nn.functional.xxx does not.
A: import torch.nn.functional as F
A: x = F.relu(x)

Q: Explain ReLU, leaky ReLU, ELU functions
A: Nonlinear activation functions (usually sigmoids) are preferred as they allow the nodes to learn 
A: more complex structures in the data. But sigmoids have limited sensitivity (around midpoint)
A: and saturate fast (going quickly to 1 and 0 or -1). Also suseptible to vanishing gradient problem.
A: Rectified linear activation unit: g(z) = max{0, z} is nearly linear, it preserves many of the 
A: properties that make linear models easy to optimize with gradient-based methods. 
A: They also preserve many of the properties that make linear models generalize well.
A: Leaky ReLU, is like ReLU, but it has a small slope for negative values instead of a flat slope
A: Unlike to ReLU, ELU can produce negative outputs.

Q: What is vanishing gradient problem?
A: Layers deep in large networks using these nonlinear activation functions fail to receive 
A: useful gradient information. Error is back propagated through the network and used 
A: to update the weights. The amount of error decreases dramatically with each additional 
A: layer through which it is propagated, given the derivative of the chosen activation function. 
A: This is called the vanishing gradient problem and prevents deep 
A: (multi-layered) networks from learning effectively.
A: Vanishing gradients make it difficult to know which direction 
A: the parameters should move to improve the cost function

Q: How to rearange dimentions in a tensor
A: Using permute function:
A: pic = torch.rand(3,32,32)
A: pic = pic.permute(1,2,0)
A: plt.imshow(pic)

Q: How to split Dataset into train and validation datasets?
A: from torch.utils.data import random_split
A: train_dataset, validation_dataset = random_split(train_and_val_dataset, [train_size, validation_size])

Q: model.train() vs model.eval() vs torch.no_grad()
Q: How to use them?
A: The first 2 set layers such as BatchNorm2d or Dropout2d from training to inference mode (wnioskowanie).
A: torch.no_grad() impacts the autograd engine and deactivate it.
A: for epoch in range(epochs):
A:   model.train()
A:     for batch in train_loader:
A:       # some code
A: 
A:    # Validation
A:    model.eval()
A:    with torch.no_grad():
A:      for batch in val_loader:
A:        #some code
A: 
A: # Testing / inference
A: model.eval()
A: with torch.no_grad():
A:    for batch in test_loader:
A:        #some code/ inference
