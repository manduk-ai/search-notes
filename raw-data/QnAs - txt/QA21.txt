Q: Implement code for sigmoid and its derivative
A: import numpy as np
A: def sigmoid(x):
A:   return 1 / (1 + np.exp(-x))
A: def deriv_sigmoid(x):
A:   return sigmoid(x) * (1 - sigmoid(x))

Q: Implement code for hiperbolic tangent and its derivative
A: import numpy as np
A: def tanh(x):
A:   return 2 / (1 + np.exp(-2 * x)) - 1
A: def deriv_tanh(x):
A:   return 1 - tanh(x) ** 2

Q: Implement code for relu and its derivative
A: import numpy as np
A: def relu(x):
A:   return np.max(0,x)
A: def deriv_relu(x):
A:   return (x > 0) * 1

Q: What's the purpose of training, validation/dev and test datasets?
A: Training dataset - for training parameters of your model
A: Validation/dev dataset - for training hyper-parameters of your model
A: Test dataset - for evaluating performance of the model

Q: What is latent space?
A: In machine learning, we use "latent space" to refer to a multi-dimensional space 
A: containing a meaningful internal representation of objects and where similar points 
A: appear closer together.
A: Latent spaces usually have a lower dimensionality than the feature space used 
A: to draw specific data points. Because of this, projecting an object in a latent space 
A: is typically a dimensionality reduction exercise.
A: The more dimensions in the latent space, the more sensitive it is to specific features 
A: from the input objects. In other words, small changes in the input data could cause 
A: significant variations in their representation in latent space. 
A: This makes high-dimensional spaces more likely to overfit than low-dimensional spaces.
A: On the other hand, the lower the latent space's dimensionality, the less sensitive 
A: to small changes in the input data. The more the encoder compresses the data, 
A: the fewer details will make it into the latent space. Low-dimensional spaces capture 
A: the essential features of the input data and are more robust to overfitting.

Q: Difference betweem Softmax and Log-Softmax
A: Advantages of using Log-Softmax over Softmax
A: a) improved numerical performance - no risk of NaN
A: b) gradient optimization - Gradient methods generally work better optimizing logp(x) than p(x) 
A: because the gradient of logp(x) is generally more well-scaled
A: It's worth noting that
A: Log-Softmax(tensor) ~ Sofmax(tensor - max(tensor))

Q: code for counting elements in a list or a list of lists
A: sum(el.nelement() for el in some_list)

Q: What is ROC curve
A: Receiver Operator Characteristic is a graphical plot used to show 
A: the diagnostic ability of binary classifiers.
A: Constructed by plotting the true positive rate (TPR) against the false positive rate (FPR)
A: TPR is the proportion of observations that were correctly predicted to be positive 
A: out of all positive observations (TP/(TP + FN))
A: FPR proportion of observations that are incorrectly predicted to be positive 
A: out of all negative observations (FP/(TN + FP))
A: Note that the ROC does not depend on the class distribution. This makes it useful for evaluating 
A: classifiers predicting rare events such as diseases or disasters. 
A: In contrast, evaluating performance using accuracy (TP + TN)/(TP + TN + FN + FP) would favor 
A: classifiers that always predict a negative outcome for rare events.

Q: What is RNN
A: Recurrent neural networks (RNN) are a type of artificial neural network that can process 
A: sequential or time series data. Their main difference from traditional networks is their ability 
A: to take information from prior inputs to influence the current input and output. 
A: This ability allows them to capture any sequential information present in the data. 
A: For example, an RNN is ideal for capturing the dependency between words of a sentence.
A: RNN processes the data sequentially so a model can process sequences of varying sizes. 
A: For example, an RNN can process a 5-word and 10-word sentence using the same input structure, 
A: unlike a traditional neural network that will need a different input size for each case.
A: ----
A: RNNs share the same weight parameters for every input sample, unlike traditional networks 
A: with different weights across each input node. Sharing parameters helps an RNN generalize 
A: to sequences of varying lengths and operate similarly on sequences with the same meaning 
A: but organized differently.

Q: What is Precision, Accuracy, Recall (sensitivity), Specificity
A: These are the main metrics used to assess performance of classification models.
A: -----
A: Accuracy: how many times the ML model was correct overall. 
A: Accuracy = (# of correct predictions) / (# of all predictons)
A: -----
A: Precision: how good the model is at predicting a specific category. 
A: Precision =  (true positives) / (all positive predictions for this category = TP + FP)
A: -----
A: Recall or Sensitivity: how many times the model was able to detect a specific category.
A: In other words - True Positive Rate. The capacity of a model to identify positive samples.
A: Recall or Sensitivity = (TP / (TP + FN)
A: Higher Recall / sensitivity: Model is good at detecting positive samples.
A: -----
A: Specificity: True Negative Rate of the model.
A: Higher specificity: Model is good at detecting negative samples.
A: Specificity = (TN) / TN + FP
