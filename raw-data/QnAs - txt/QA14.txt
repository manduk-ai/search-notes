Q: What is Gradient Descent?
A: Gradient Descent is an optimization algorithm for finding 
A: a local minima of a differentiable function.
A: In machine learning, we use GD to minimize cost / loss function.
A: We do that iteratively:
A: Compute the gradient (slope), the first order derivative of the function at that point
A: Make a step in the direction opposite to the gradient

Q: Explain difference between GD and: Batch GD, SGD, Mini-Batch GD
A: Batch GD
A: We take the average of the gradients of all the training examples 
A: and then use that mean gradient to update our parameters. 
A: So thatâ€™s just one step of gradient descent in one epoch.
A: Stochastic GD 
A: We consider just one example at a time to take a single step
A: SGD can be used for larger datasets. 
A: It converges faster when the dataset is large 
A: as it causes updates to the parameters more frequently.
A: We cannot implement the vectorized implementation for SDG
A: Mini-Batch GD
A: We don't use all data at once nor we use the single example at a time. 
A: We use a batch of a fixed number of training examples which is less 
A: than the actual dataset and call it a mini-batch

Q: How to import Dataset and DataLoader?
A: from torch.utils.data import Dataset, DataLoader

Q: How to import Linear model and SGD optimizer?
A: from torch.nn import Linear
A: from torch.optim import SGD

Q: How SGD optimizer works in PyTorch?
A: SGD optimizer in PyTorch actually is Mini-batch Gradient Descent.
A: But you define the size of the mini-batch in the data loader, not in the optimizer.

Q: criterion = torch.nn.BCELoss()
Q: loss = criterion(? , ?)
Q: Which is the first parameter: prediction or target
A: loss = criterion(prediction, target)
A: Otherwise expect 'nan' as a loss value

Q: Difference between torch.tensor() and torch.Tensor?
A: torch.Tensor is alias for torch.FloatTensor 
A: torch.tensor infers the dtype automatically
A: it's better to use torch.tensor as it also has arguments like dtype
A: if you would like to change the type.

Q: What sklearn module provides us with generators of data of different shapes?
A: datasets
A: from sklearn import datasets
A: make_blobs
A: make_circles

Q: How to create 2D numpy array so it looks like this?
Q: [ 1,  2,  3,  4,  5
Q:   6,  7,  8,  9, 10]
A: np.array([[1,2,3,4,5],[6,7,8,9,10]])
A: array([[ 1,  2,  3,  4,  5],
A:       [ 6,  7,  8,  9, 10]])

Q: From where do you import Dataset and DataLoader?
A: from torch.utils.data import Dataset, DataLoader
