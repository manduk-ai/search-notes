Q: How to initialize tensor in PyTorch(4 methods)
A: From data variable: torch.tensor(data)
A: From numpy array:  torch.from_numpy(np_array)
A: From another tensor: torch.ones_like(x_data); torch.rand_like(x_data, dtype=torch.float)
A: Random, zeros and ones:torch.rand(shape); torch.ones(shape); torch.zeros(shape)

Q: What are PyTorch operations with _, like tensor.add_(5)
A: In-place operations -> result stored in operand
A:  Discouraged as problematic with derivatives

Q: PyTorch: can tensors and numpy arrays share memory
A: Yes, if on CPU. Change in tensor reflects in numpy array

Q: PyTorch. DataSet vs DataLoader classes - what do we use them for?
A: A DataSet is designed for retrieval of individual data items 
A: while a DataLoader is designed to work with batches of data.

Q: PyTorch. How to create class for neural network?
A: By subclassing nn.Module, and initialize the neural network layers in __init__. 
A: Every subclass implements the operations on input data in the forward method.
A: class NeuralNetwork(nn.Module):
A:     def __init__(self):
A:     (...)
A:     def forward(self, x):
A:     (..)

Q: PyTorch. What are logits?
A: Raw values in [-infty, infty] that are returned by the last linear layer of the neural network
A: and then passed to the nn.Softmax module

Q: Explain process of training a neural network
A: Training a model is an iterative process; in each iteration (called an epoch) the model 
A: makes a guess about the output, calculates the error in its guess (loss),
A: collects the  derivatives of the error with respect to its parameters, 
A: and optimizes these parameters using gradient descent.
