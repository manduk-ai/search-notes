{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Th goal of this notebook is to..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to set up a number of variables we going to use later in the notebook:\n",
    "- OPENAI_API_KEY: to access the OpenAI embeddings (secret)\n",
    "- AZURESEARCH_ADMIN_KEY: to access the Azure Search service (secret)\n",
    "- AZURESEARCH_ENDPOINT (theoretically it's not a secret, but it's better to keep it in .env file)\n",
    "- AZURESEARCH_INDEX_NAME (theoretically it's not a secret, but it's better to keep it in .env file)\n",
    "- if you create index in Azure Portal (not programatically), the default field names will be different than the ones assumed by the AzureSearch in langchain_community package. You can set the following variables to match the field names in your index:\n",
    "    - AZURESEARCH_FIELDS_CONTENT_VECTOR=text_vector (it's not a secret) \n",
    "    - AZURESEARCH_FIELDS_ID=chunk_id (it's not a secret)\n",
    "    - AZURESEARCH_FIELDS_CONTENT=chunk (it's not a secret)\n",
    "    \n",
    "All above will be read from .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need is to build a retriever. \n",
    "\n",
    "\n",
    "We will use Microsoft AI Search with hybrid mode to retrieve the most relevant documents for a given query.\n",
    "\n",
    "Source code for langchain_community.vectorstores.azuresearch: \n",
    "https://api.python.langchain.com/en/latest/_modules/langchain_community/vectorstores/azuresearch.html#AzureSearch.similarity_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "# OpenAI API data (for embeddings)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_api_version = \"2023-05-15\"\n",
    "model = \"text-embedding-ada-002\"\n",
    "\n",
    "# Azure AI Search data (for vector store)\n",
    "vector_store_address = os.getenv(\"AZURESEARCH_ENDPOINT\") \n",
    "vector_store_password = os.getenv(\"AZURESEARCH_ADMIN_KEY\")\n",
    "vector_store_index = os.getenv(\"AZURESEARCH_INDEX_NAME\")\n",
    "\n",
    "# Initialize the embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=openai_api_key, \n",
    "    openai_api_version=openai_api_version, \n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Initialize the vector store\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=vector_store_index,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever(k=4)\n",
    "# AISearch retriver https://python.langchain.com/v0.2/docs/integrations/retrievers/azure_ai_search/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is collections module and give examples.\n",
      "This module implements specialized container datatypes providing alternatives \n",
      "to Python’s general purpose built-in containers, dict, list, set, and tuple.\n",
      "Ex.1: Counter – dict subclass for counting hashable objects\n",
      "Ex.2: defaultdict – dict subclass that calls a factory function to supply missing values\n",
      "Ex.3: namedtuple – factory function for creating tuple subclasses with named fields\n",
      "####\n",
      "\n",
      "What are generator expression and why use them?\n",
      "gen = (costly_fn(data) for data in iterable)\n",
      "next(gen)\n",
      "They behave a bit like a \"lazy list comprehension.\"\n",
      "Use them if you want data-on-demand, but you might not want to compute all of it at once in memory\n",
      "As a contract needle in [expensive_fn(item) for item in haystack]\n",
      "will evaluate the entire list comprehension, and only then begins the search\n",
      "Note generator expressions use parentheses instead of square brackets\n",
      "####\n",
      "\n",
      "What would be the output of this code?\n",
      "ex = (x ** 2 for x in range(10))\n",
      "16 in ex\n",
      ">>> ???\n",
      "list(ex)\n",
      ">>> ???\n",
      "True\n",
      "[25, 36, 49, 64, 81] - the rest of the generator expression\n",
      "####\n",
      "\n",
      "What is a generator function?\n",
      "It's like a normal function, except it contains the keyword yield.\n",
      "When called, a generator function returns a generator iterator \n",
      "that can produce subsequent values on demand by running the function \n",
      "until it encounters a yield statement, and then pausing.\n",
      "def func(n):\n",
      "   for i in range(100):\n",
      "      yield i**n\n",
      "f = func(5)\n",
      "next(f)\n",
      "####\n",
      "\n",
      "What is a decorator and what are use cases?\n",
      "A decorator is a transformation that can be applied to a function\n",
      "They can be used to:\n",
      "- handle shared behavior to append to functions, like printing argument\n",
      "- cache return values to increase performance\n",
      "- set a timeout on blocking functions\n",
      "- mark class properties as \"read-only\"\n",
      "- mark methods as static methods or class methods\n",
      "- define event-driven handlers (for GUIs, or web-based clients)\n",
      "####\n",
      "24 Jan 2024, 09:51:09\n",
      "\n",
      "Function Calling template\n",
      "\n",
      "Function calling is one of language models' most powerful feat, yet many don't know how to use them.\n",
      "\n",
      "Function calling is the process where the model executes a specific set of instructions or a subroutine, known as a function, in response to a request or a command. \n",
      "\n",
      "This is useful for performing specialized tasks, such as calculations, data retrieval, or operating external tools, that are beyond the basic text generation capabilities of the LLM. Every production-grade LLM app probably has one or more customized functions.\n",
      "\n",
      "For example, when a user asks for the current weather in a specific city, the LLM calls a function that accesses a weather API, retrieves the latest weather data for that city, and then returns this information to the user in a comprehensible format.\n",
      "\n",
      "Many struggle to start with function calling as it requires serializing the customized functions before sending them to a language model such as GPT-4.\n",
      "\n",
      "I recently came across a ready-to-use code snippet that you can easily copy-paste into your project for immediate application (h/t Anton Bacaj) https://lnkd.in/gp5HGJt6\n",
      "\n",
      "\n",
      "Functions\n",
      "13 Dec 2023, 22:03:40\n",
      "\n",
      "Benchmarked function calling\n",
      "\n",
      "https://twitter.com/robertnishihara/status/1734629320868687991?s=51&t=cIR-2pnv_S6WYIaT1JI-AA\n",
      "\n",
      "\n",
      "\n",
      "Functions\n",
      "15 Oct 2023, 10:51:05\n",
      "\n",
      "Generative AI - How it works?\n",
      "\n",
      "https://ig.ft.com/generative-ai/\n",
      "\n",
      "\n",
      "Strategy\n"
     ]
    }
   ],
   "source": [
    "# query = \"What are the fully local agents?\"\n",
    "# query = \"What is Nvidia NIM API?\"\n",
    "query = \"What is a generator function?\"\n",
    "docs = retriever.invoke(query)\n",
    "for doc in docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://github.com/langchain-ai/langchain/discussions/18752\n",
    "\n",
    "prompt = \"Why is the sky blue?\"\n",
    "\n",
    "# Use a \"regular\" similarity search, but with the hybrid option\n",
    "docs = vector_store.similarity_search(\n",
    "    query=prompt,\n",
    "    k=10,\n",
    "    search_type=\"hybrid\",\n",
    "    filters=f\"source eq 'A'\" #If you want to use filters\n",
    ")\n",
    "\n",
    "# Create the RAG chain\n",
    "rag_chain_from_docs = (\n",
    "    RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Prepare the input for the RAG chain\n",
    "rag_input = {\"query\": prompt, \"context\": docs}\n",
    "\n",
    "# Generate the answer using the RAG chain\n",
    "rag_output = rag_chain_from_docs.invoke(rag_input)\n",
    "\n",
    "# Create the final output dictionary manually\n",
    "answer = {\n",
    "    \"context\": docs,\n",
    "    \"question\": prompt,\n",
    "    \"answer\": rag_output\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpler solution:\n",
    "# https://stackoverflow.com/questions/78576496/hybrid-search-using-azure-ai-search-and-lang-chain-as-a-retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "searchnotes_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
