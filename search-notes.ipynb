{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to set up a number of variables we going to use later in the notebook:\n",
    "- OPENAI_API_KEY: to access the OpenAI embeddings (secret)\n",
    "- AZURESEARCH_ADMIN_KEY: to access the Azure Search service (secret)\n",
    "- AZURESEARCH_ENDPOINT (theoretically it's not a secret, but it's better to keep it in .env file)\n",
    "- AZURESEARCH_INDEX_NAME (theoretically it's not a secret, but it's better to keep it in .env file)\n",
    "- if you create index in Azure Portal (not programatically), the default field names will be different than the ones assumed by the AzureSearch in langchain_community package. You can set the following variables to match the field names in your index:\n",
    "    - AZURESEARCH_FIELDS_CONTENT_VECTOR=text_vector (it's not a secret) \n",
    "    - AZURESEARCH_FIELDS_ID=chunk_id (it's not a secret)\n",
    "    - AZURESEARCH_FIELDS_CONTENT=chunk (it's not a secret)\n",
    "    \n",
    "All above will be read from .env file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = load_dotenv(find_dotenv(filename='.env'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need is to build a retriever. \n",
    "\n",
    "\n",
    "We will use Microsoft AI Search with hybrid mode to retrieve the most relevant documents for a given query.\n",
    "\n",
    "Source code for langchain_community.vectorstores.azuresearch: \n",
    "https://api.python.langchain.com/en/latest/_modules/langchain_community/vectorstores/azuresearch.html#AzureSearch.similarity_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.azuresearch import AzureSearch\n",
    "\n",
    "# OpenAI API data (for embeddings)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai_api_version = \"2023-05-15\"\n",
    "model = \"text-embedding-ada-002\"\n",
    "\n",
    "# Azure AI Search data (for vector store)\n",
    "vector_store_address = os.getenv(\"AZURESEARCH_ENDPOINT\") \n",
    "vector_store_password = os.getenv(\"AZURESEARCH_ADMIN_KEY\")\n",
    "vector_store_index = os.getenv(\"AZURESEARCH_INDEX_NAME\")\n",
    "\n",
    "# Initialize the embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    openai_api_key=openai_api_key, \n",
    "    openai_api_version=openai_api_version, \n",
    "    model=model\n",
    ")\n",
    "\n",
    "# Alternatively, we may use AzureOpenAIEmbeddings\n",
    "# from langchain_openai import AzureOpenAIEmbeddings\n",
    "# embeddings = AzureOpenAIEmbeddings(\n",
    "#     model=\"<embeding-model>\",\n",
    "#     azure_endpoint=\"<Azure_openai_endpoint>\",\n",
    "#     azure_ad_token_provider=token_provider\n",
    "# )\n",
    "\n",
    "# Initialize the vector store\n",
    "vector_store = AzureSearch(\n",
    "    azure_search_endpoint=vector_store_address,\n",
    "    azure_search_key=vector_store_password,\n",
    "    index_name=vector_store_index,\n",
    "    embedding_function=embeddings.embed_query,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There multiple way of seting up the retriever, the issue here is to initialize it this way that is uses hybrid not only similarity search\n",
    "# I use this one: https://stackoverflow.com/questions/78576496/hybrid-search-using-azure-ai-search-and-lang-chain-as-a-retriever\n",
    "retriever = vector_store.as_retriever(k=2, search_type=\"hybrid\")\n",
    "\n",
    "# There is also Azure AI Search retriever https://python.langchain.com/v0.2/docs/integrations/retrievers/azure_ai_search/\n",
    "# Here is the doc: https://python.langchain.com/v0.2/docs/integrations/retrievers/azure_ai_search/\n",
    "\n",
    "# Another way discussed here: https://github.com/langchain-ai/langchain/discussions/18752"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some example queries\n",
    "# query = \"What are the fully local agents?\"\n",
    "# query = \"What is Nvidia NIM API?\"\n",
    "query = \"What is a generator function?\"\n",
    "docs = retriever.invoke(query)\n",
    "for doc in docs:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a main chain - it will use retrieved documents to generate an answer\n",
    "# We also define two versions of the chain - one with Ollama (llama3.1) and one with OpenAI (gpt-4o-mini)\n",
    "\n",
    "# PROVIDER = \"ollama\"\n",
    "PROVIDER = \"openai\"\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    \n",
    "    Analyze carefully and use the following documents to answer the user question. \n",
    "    \n",
    "    If you don't know the answer, just say that you don't know. Do not make up an answer. \n",
    "    \n",
    "    Keep your answer short and to the point.\n",
    "    \n",
    "    Question: {question} \n",
    "    Documents: {documents} \n",
    "    Answer: \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"documents\"],\n",
    ")\n",
    "\n",
    "if PROVIDER == \"openai\":\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=0,\n",
    "    )\n",
    "else:\n",
    "    llm = ChatOllama(\n",
    "        model=\"llama3.1\",\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "main_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define websearch tool - we use Tavilyn for this\n",
    "from langchain.schema import Document\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "searchnotes_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
